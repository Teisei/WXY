# -*- coding: utf-8 -*-
import os
import re

from whoosh.index import create_in, open_dir
from whoosh.fields import *
from whoosh.qparser import MultifieldParser
from whoosh.sorting import FieldFacet
import jieba
from jieba.analyse import ChineseAnalyzer
import jieba.analyse
import json
import html

# tp = r'\n.\n'
# a = """
# ä½ å¥½ï¼š
# æˆ‘
# æ— è¯­
# """
# x = re.findall(tp, a)

STOPWORDS = [a for a in 'ï¼Œã€ã€‚ï¼ï¼Ÿï¼›ï¼šâ€œâ€â€˜â€™ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€ã€‘â€¦â€¦â€”Â·ï½']
jieba.load_userdict('indexdata/stopwords.dict.txt')
jieba.analyse.set_stop_words('indexdata/stopwords.dict.txt')
BAD_SENTENCE_PATTERN = [
    r'ç¾¤.*\d.*\d.*\d.*\d.*\d.*\d.*\d'
]
def flat_sentence(sentence):
    # ç‰¹æ®Šå­—ç¬¦è¿‡æ»¤
    for pattern in BAD_SENTENCE_PATTERN:
        matches = re.findall(pattern, sentence)
        if matches:
            return ''
    return sentence
    # while '  ' in s.strip():
    #     s = s.replace('  ', ' ')


def replace_html_entities(text):
    # å®šä¹‰éœ€è¦æ›¿æ¢çš„ç‰¹æ®Šå­—ç¬¦å’Œå¯¹åº”çš„æ›¿æ¢å€¼
    html_entities = {
        '<br>': '\n',
        '<br />': '\n',
        '<p>': '\n',
        '</p>': '\n'
    }
    pattern = re.compile('|'.join(html_entities.keys()))
    replaced_text = pattern.sub(lambda m: html_entities[m.group(0)], text)

    return replaced_text

def is_meaningful_line(text):
    pattern = r'^[^\u4e00-\u9fa5a-zA-Z.]+$'
    if re.search(pattern, text):
        return False
    else:
        return True

def flat_content(title, text):
    # ç½‘é¡µç‰¹æ®Šå­—ç¬¦æ›¿æ¢
    text1 = html.unescape(text)
    text1 = html.unescape(text1)
    text1 = html.unescape(text1)
    text1 = replace_html_entities(text1)

    # è¿‡æ»¤æ˜æ˜¾æœ‰é—®é¢˜çš„è¡Œ
    text1 = text1.replace('â€¦', 'â€¦\n')
    text1 = text1.replace('...', '...\n')
    text1 = text1.replace('â€»â€»â€»', 'â€»â€»â€»\n')
    text1 = text1.replace('===', '===\n')
    text1 = text1.replace('___', '___\n')
    text1 = text1.replace('---', '---\n')
    text1 = text1.replace('ï¼Šï¼Šï¼Š', 'ï¼Šï¼Šï¼Š\n')
    text1 = text1.replace('â€”â€”â€”', 'â€”â€”â€”\n')
    text1 = text1.replace('~~~', '~~~\n')
    lines = [a for a in text1.split('\n') if is_meaningful_line(a)]
    text2 = '\n'.join(lines)

    # # å¤„ç†æ¢è¡Œ
    # tp = r'\n..\n'
    # tp = r'\n'
    # x = re.findall(tp, text2)
    # if x:
    #     print(x)
    #     print("\n\n\n{}\n=== beforeã€{}ã€‘\n=== after ã€{}ã€‘".format(title, text, text2))

    # æ–­å¥

    # åˆ†å¥å­å¤„ç†
    sentences = [e.strip() for e in re.split(r'[ã€‚ï¼ï¼Ÿ\n\r]', text2)]
    sentences = [flat_sentence(e.strip()) for e in sentences]
    sentences = [a for a in sentences if a != '']
    text3 = '\n'.join(sentences).strip()

    # tp = r'\n.*\n'
    # x = re.findall(tp, text)
    # if x:
    #     print("\n\n{}\nã€Š{}ã€‹\n=== beforeã€{}ã€‘\n=== after ã€{}ã€‘".format(x, title, text, text3))

    return 'ï¼›'.join(sentences)
    # # åˆ†å¥å­
    # seg_list = jieba.cut(text)
    # word1 = [w for w in seg_list]
    # word2 = [word1[0]]
    # for seg in word1[1:]:
    #     if seg != word2[-1]:
    #         word2.append(seg)
    # return word2

    # # è¿‡æ»¤åœç”¨è¯å’Œå•ä¸ªå­—ç¬¦
    # filtered_words = [word for word in seg_list if word not in STOPWORDS and len(word) > 1]
    # text = '|'.join(filtered_words)
    # return '\n'.join(arr2).strip()
def parse_one_detail(line):
    if line.strip() == 'ERROR':
        return {}
    book_json = json.loads(line.strip())
    book = book_json['state']['Book']
    bookInfo = book['bookInfo']
    title = bookInfo['title']
    className = bookInfo['classInfo']['className']
    author = bookInfo['author']
    score = bookInfo['score']
    scorerCount = bookInfo['scorerCount']
    scoreDetail = bookInfo['scoreDetail']
    tags = bookInfo['tags']
    tags.append(className.strip())
    tags = list(set([t.strip() for t in tags]))
    tags_as_str = ','.join(tags)
    status = bookInfo['status']  # 0 è¿è½½ä¸­ï¼Œ1 å·²å®Œç»“
    introduction = bookInfo['introduction']
    # a = introduction.strip()
    # b = flat_content(title, a)
    # print("\n\n\n=== beforeã€{}ã€‘\n=== after ã€{}ã€‘".format(a, b))

    countWord = bookInfo['countWord']
    cover = bookInfo['cover']
    # scores = book['scoreDetail']
    # tags = book['tags']

    sources = book['bookSource']
    jump_link = sources[0]['phonePage'] if len(book['bookSource']) > 0 else 'æš‚æ— é“¾æ¥'
    booklists = book['booklistsResult']['booklists']  # åªæœ‰ç¬¬ä¸€é¡µï¼Œæœ€å¤š20ç¯‡
    # assert len(booklists) == book['booklistsResult']['total']
    assert len(booklists) <= 20

    comments = book['commentsResult']['comments']
    # assert len(comments) == book['commentsResult']['total']
    assert len(comments) <= 20  # åªæœ‰ç¬¬ä¸€é¡µï¼Œæœ€å¤š20ç¯‡


    return {
        "title": title.strip(),
        "author": author.strip(),
        "score": score,
        "scorerCount": scorerCount,
        "className": className.strip(),
        "tags": tags_as_str.strip(),
        "desc": flat_content(title, introduction),
        "url": jump_link
    }

def load_books():
    uid_list = []
    books = []
    indexdata_path = 'indexdata'
    for root, dirs, files in os.walk(indexdata_path):
        for file in files:
            file_path = os.path.join(root, file)
            if file_path.endswith('_DETAIL.txt'):
                with open(file_path, 'r', encoding='utf-8') as f:
                    for _ in f.readlines():
                        book = parse_one_detail(_)
                        if 'title' in book:
                            uid = '{}###{}'.format(book['title'], book['author'])
                            if not uid in uid_list:
                                books.append(book)
                                uid_list.append(uid)
                            else:
                                print('find ' + uid)
                print('loaded from {}, total document count {}.'.format(file_path, len(books)))
    return books

def build(books):
    jieba.load_userdict("./indexdata/semantic_labels.dict.txt")

    # åˆ›å»ºschema, storedä¸ºTrueè¡¨ç¤ºèƒ½å¤Ÿè¢«æ£€ç´¢
    schema = Schema(
        title=TEXT(stored=True),
        author=ID(stored=True),
        score=NUMERIC(stored=True, sortable=True),
        scorerCount=NUMERIC(stored=True, sortable=True),
        className=ID(stored=True),
        tags=KEYWORD(stored=True, analyzer=ChineseAnalyzer()),
        desc=TEXT(stored=True, analyzer=ChineseAnalyzer()),
        url=ID(stored=True)
    )
    # å­˜å‚¨schemaä¿¡æ¯è‡³indexdirç›®å½•
    indexdir = 'indexdir/'
    if not os.path.exists(indexdir):
        os.mkdir(indexdir)
        # ä½¿ç”¨create_inæ–¹æ³•åˆ›å»ºç´¢å¼•ï¼Œindex_pathä¸ºç´¢å¼•è·¯å¾„ï¼Œschemaä¸ºå‰é¢å®šä¹‰çš„ç´¢å¼•å­—æ®µï¼Œindexnameä¸ºç´¢å¼•åç§°ï¼ˆæ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ï¼‰
        ix = create_in(indexdir, schema=schema, indexname='indexname')

    writer = ix.writer()
    for book in books:
        writer.add_document(
            title=book['title'],
            author=book['author'],
            score=book['score'],
            scorerCount=book['scorerCount'],
            className=book['className'],
            tags=book['tags'],
            desc=book['desc'],
            url=book['url']
        )
    writer.commit()

def test_query(keyword):
    EMOJI_ALL = 'ğŸ§ ğŸ‘€ğŸ‘ï¸ğŸ¦¾âœï¸ğŸ¦´ğŸ«ğŸ‘½ğŸ‘»ğŸ‘ï¸â€ğŸ—¨ï¸ğŸ¾ğŸˆâ€â¬›ğŸª¶ğŸ•¸ï¸ğŸ•·ï¸ğŸ«ğŸ¥ğŸŒğŸŒ„ğŸŒƒğŸŒ†ğŸŒ‰ğŸªğŸ¡ğŸš¨ğŸš‚ğŸš‡ğŸ§³ğŸ›¸ğŸ›ï¸âŒšâ³ğŸ•˜ğŸ•°ï¸ğŸƒğŸğŸƒğŸ§¸ğŸªğŸª†ğŸª„ğŸ¨ğŸ‘“ğŸ‘œğŸ‘ğŸ’¾ğŸï¸ğŸ“¹ğŸ“ºğŸ“¼ğŸ“·ğŸ¥ğŸ”ğŸ”ğŸ”¦ğŸ•¯ï¸ğŸ“ƒğŸ“œğŸ“°ğŸ—ï¸âœ‰ï¸ğŸ“¦ğŸ“®ğŸ–‹ï¸ğŸ“†ğŸ“ğŸ’¼ğŸ“ğŸ—„ï¸ğŸ”’ğŸ—ï¸ğŸ§ªğŸ’‰ğŸ©¸ğŸ’Š'
    EMOJI_BY_CLASSNAME = {
        "ç„å¹»": 'ğŸ‘¹ğŸª½â›©ï¸â˜˜ï¸ğŸ€ğŸ‚ğŸ”ï¸ğŸŒ‹ğŸœï¸ğŸ—»ğŸï¸ğŸ›ï¸ğŸ¯ğŸ°â›ªğŸ•ŒğŸ•‹ğŸ•ğŸ›•â™¨ï¸ğŸŒªï¸ğŸŒ‘ğŸğŸğŸ¥‹ğŸ´ğŸ”®ğŸ­ğŸ‘‘ğŸ‘˜ğŸ“¿ğŸ¥»ğŸª­ğŸª‡ğŸª•ğŸ®ğŸ•¯ï¸ğŸª”ğŸ“œğŸ“•ğŸ’°ğŸ’¸ğŸª™âš”ï¸ğŸ¹ğŸ—¡ï¸ğŸ›¡ï¸ğŸªğŸ§ªğŸªâš°ï¸ğŸ—¿âš±ï¸ğŸ§¿ğŸª¦ğŸª¬â˜ªï¸â˜¯ï¸â˜®ï¸ğŸ›â™ˆâ™Œâ™‹âšœï¸âš•ï¸ğŸ”°ğŸ”±ğŸ’ ğŸ´â€â˜ ï¸ğŸ¥·'
        , "æ‚¬ç–‘": EMOJI_ALL
    }

    def _get_emoji(className):
        import random
        flag = 'ğŸ‘‰'
        if className in EMOJI_BY_CLASSNAME:
            flag = EMOJI_BY_CLASSNAME[className][random.randrange(len(EMOJI_BY_CLASSNAME[className]))]
        return flag

    current_path = os.getcwd()
    indexdir = os.path.join(current_path, 'indexdir/')
    ix = open_dir(indexdir, indexname='indexname')
    searcher = ix.searcher()
    parser = MultifieldParser(["title", "author", "desc"], ix.schema).parse(keyword)
    facet = FieldFacet("scorerCount", reverse=True)
    # limitä¸ºæœç´¢ç»“æœçš„é™åˆ¶ï¼Œé»˜è®¤ä¸º10ï¼ŒNoneä¸ºä¸é™åˆ¶ã€‚sortedbyä¸ºæ’åºè§„åˆ™
    results = searcher.search(parser, limit=None, sortedby=facet, terms=True)
    print('\nä¸€å…±å‘ç°{}ä»½ã€{}ã€‘æ–‡æ¡£ã€‚'.format(len(results), keyword))
    for i in range(min(9999, len(results))):
        # æ ‡é¢˜
        print('ã€Š{}ã€‹{}è‘—ã€‚{}ã€‚{}ã€‚è¯„åˆ†:{}ã€‚{}'.format(
            results[i].fields()['title'],
            results[i].fields()['author'],
            _get_emoji(results[i].fields()['className']),
            # results[i].fields()['className'],
            results[i].fields()['tags'],
            results[i].fields()['score'],
            results[i].fields()['desc'])
        )
        # print((json.dumps(results[i].fields(), ensure_ascii=False)))
    ix.close()

if __name__ == '__main__':
    # books = load_books()
    # build(books)
    test_query('å‡¡äººä¿®ä»™ä¼ ')
    test_query('é®å¤©')
    test_query('çˆ±æ½œæ°´çš„ä¹Œè´¼')
    test_query('è¯¡ç§˜')
    test_query('èµç¾æ„šè€…')
    test_query('æ„šè€…å…ˆç”Ÿ')
    test_query('è¯¡ç§˜ä¹‹ä¸»')
    test_query('æ–—ç ´è‹ç©¹')
    test_query('å¤©èš•åœŸè±†')
    test_query('æ–—ç ´')
    test_query('çƒ½ç«æˆè¯¸ä¾¯')
    test_query('é›ªä¸­æ‚åˆ€è¡Œ')


    # test_query('éƒ½é‡ç”Ÿäº†è°è°ˆæ‹çˆ±å•Š')
    # test_query('å‡ºåå¤ªå¿«æ€ä¹ˆåŠ')
    # test_query('ä¸‡å¤ç¥å¸')
    # test_query('æ–—æ°”')
    # test_query('å²ä¸Šæœ€ç‰›å®—é—¨')
    #
    # test_query('æ€»è£')
    # test_query('éœ¸æ€»')
    # test_query('éœ¸é“æ€»è£')
    # test_query('è´±äºº')
    # test_query('å®«æ–—')
    print('test')